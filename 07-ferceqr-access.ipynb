{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b735222-167b-47b5-908f-a2667c23e8e7",
   "metadata": {},
   "source": [
    "# FERC Electric Quarterly Report (EQR) Access Examples\n",
    "\n",
    "### Background\n",
    "The Electric Quarterly Report (EQR) is submitted by sellers participating in bilateral electricity market transactions. The reports summarize the contractual terms and conditions in agreements for all jurisdictional services, including cost-based sales, market-based rate sales, and transmission service, as well as transaction information for short-term and long-term market-based power sales and cost-based power sales.\n",
    "\n",
    "We've known that users are interested in this dataset for some time, but have been deterred by the large size relative to other datasets in PUDL. However, with the help of modern tooling like [DuckDB](https://duckdb.org/), we've finally been able to make an initial version of the EQR dataset available in PUDL. This initial version applies some basic cleaning to the raw data, but is primarily focused on just making the data accessible. We hope to work with users going forward to identify high-priority targets for quality improvements, so please reach out if you discover any irregularities within the data!\n",
    "\n",
    "EQR data is comprised of the following 4 tables:\n",
    "\n",
    "- `core_ferceqr__contracts`\n",
    "- `core_ferceqr__quarterly_identity`\n",
    "- `core_ferceqr__quarterly_index_pub`\n",
    "- `core_ferceqr__transactions`\n",
    "\n",
    "For table and column level metadata see our [docs](https://catalystcoop-pudl.readthedocs.io/en/latest/data_sources/ferceqr.html#ferc-form-920-electric-quarterly-report-eqr).\n",
    "\n",
    "### Working with EQR\n",
    "Both the size and structure of the EQR dataset makes working with this data slightly different, and in some ways more difficult than accessing other PUDL tables. All of the tables have a partitioned structure, where one table is actually comprised of a directory of parquet files (as opposed to one parquet file containing the entire table as with other PUDL tables). While this partitioned structure is new to PUDL, it is quite common with parquet data and should be handled well by typical tools. The size of the dataset, however, presents a more significant challenge. Fortunately, this is only an issue for the `core_ferceqr__transactions` table, which is >83GB in total. The other tables are all small enough that they can be loaded in their entirety into memory on almost any computer.\n",
    "\n",
    "This notebook will demonstrate how to access EQR data using several standard Python tools, and provide some tips for working with the big transactions table without crashing your computer. The specific tools we will be using are below:\n",
    "\n",
    "- `pandas` - go to dataframe library in Python, but not particularly well suited for working with large data\n",
    "- `duckdb` - a fast, memory efficient SQL database system with a Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af101a2c-8a1c-4ead-b84e-231fca209c6e",
   "metadata": {},
   "source": [
    "## Basic access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711fb569-6a9b-42da-979d-9c9c755b9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ident = pd.read_parquet(\"s3://pudl.catalyst.coop/ferceqr/core_ferceqr__quarterly_identity/\")\n",
    "ident.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c995b4-271e-400b-ab06-dcb27cea171e",
   "metadata": {},
   "source": [
    "This snippet above demonstrates how easy it is to query data from a partitioned table like these. Passing a directory to the `read_parquet` function works without any special considerations. It also gives us some insight into the data contained in the `identity` table. This table contains data specific to the individual who filed EQR on behalf of a company for a specific year-quarter. This table is probably not particularly interesting in most contexts, so let's take a look at another table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392263a8-f491-43b0-b989-aecbb14d9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts = pd.read_parquet(\"s3://pudl.catalyst.coop/ferceqr/core_ferceqr__contracts/\")\n",
    "contracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565e437-aa10-409f-9a74-c4b9a5205d64",
   "metadata": {},
   "source": [
    "The `contracts` table contains information about contracts between companies selling and buying electricity market products. Unfortunately, we only have unique identifiers for the selling companies, while the purchasing companies are identified only by free-form strings. We have significant experience performing record linkage to assign unique ID's to entities like these, but we have not yet had the time or resources to devote to this problem. Despite this limitation, there's still a wealth of information contained in this table. For example, we can use this table to examine the distribution of types of products sold in a single Balancing Authority (BA) over a given time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39141c8c-5905-4890-8b3c-b3cab8e00382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "ba_name = \"PJM\"\n",
    "date_range_start = datetime.date(2017, 8, 1)\n",
    "date_range_end = datetime.date(2018, 11, 1)\n",
    "\n",
    "contracts_filtered = contracts.loc[\n",
    "    (contracts[\"point_of_delivery_balancing_authority\"] == ba_name)\n",
    "    & (contracts[\"contract_execution_date\"] >= date_range_start)\n",
    "    & (contracts[\"contract_execution_date\"] <= date_range_end)\n",
    "]\n",
    "\n",
    "contracts_filtered[\"product_type_name\"].value_counts().plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862e659-6514-4317-9e5d-9b5e5c00f30c",
   "metadata": {},
   "source": [
    "It's clear that the vast majority of these contracts are classified as `MB`, and we can check the [data dictionary](https://catalystcoop-pudl.readthedocs.io/en/latest/data_dictionaries/pudl_db.html#core-ferceqr-contracts) to see the description of this code (and others):\n",
    "\n",
    "```\n",
    "MB: Energy, capacity or ancillary services sold under the sellerâ€™s FERC-approved market-based rate tariff.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad04bf-2627-453d-8095-aa4c4518ac9e",
   "metadata": {},
   "source": [
    "## Accessing the BIG Transactions Table\n",
    "Now that we've demonstrated access to the smaller tables, we'll dive into working with the large transactions table. This can still be done using Pandas, but we won't be able to load the entire table into memory, so we will need to add some filters to our initial query. As an example, we'll take the filters used above on the `contracts` table, but we'll need an even more restrictive date range to avoid running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec638ec-3193-4457-a18c-ca8e8a38167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range_end = datetime.date(2017, 9, 1)\n",
    "\n",
    "transactions = pd.read_parquet(\n",
    "    \"s3://pudl.catalyst.coop/ferceqr/core_ferceqr__transactions/\",\n",
    "    filters=[\n",
    "        (\"trade_date\", \">=\", date_range_start),\n",
    "        (\"trade_date\", \"<=\", date_range_end),\n",
    "        (\"point_of_delivery_balancing_authority\", \"=\", ba_name)\n",
    "    ],\n",
    ")\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8939863-74e1-457f-bd1a-0302322d7818",
   "metadata": {},
   "source": [
    "This works just fine if you only need to work with a small portion of the table at a time, but will quickly run up against memory limitations if you ever need to access a larger chunk of the table. This is where tools like `DuckDB` and [Polars](https://docs.pola.rs/) come in. For this tutorial we'll focus on `DuckDB` for simplicity, and because we've been particularly impressed with `DuckDB's` performance. We can recreate the query above with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13554c-16fe-4446-b810-20f0997f0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "(\n",
    "    duckdb.read_parquet(\"s3://pudl.catalyst.coop/ferceqr/core_ferceqr__transactions/*.parquet\")\n",
    "    .filter(f\"trade_date BETWEEN DATE '{date_range_start}' AND DATE '{date_range_end}'\")\n",
    "    .filter(f\"point_of_delivery_balancing_authority = '{ba_name}'\")\n",
    "    .limit(5)\n",
    "    .fetchdf()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545596d-1431-4929-b5f9-46f13f43982e",
   "metadata": {},
   "source": [
    "`DuckDB` is even capable of operating on the entire table using its streaming SQL engine to avoid ever loading the full table into memory.\n",
    "\n",
    "As an example, we'll examine the distribution of values in the column `class_name`, which identifies transactions as firm (F), non-firm (NF), or unit power sale (UP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e298f-49d3-47a2-8f60-c6de5aeffccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    duckdb.read_parquet(\"s3://pudl.catalyst.coop/ferceqr/core_ferceqr__transactions/*.parquet\")\n",
    "    .value_counts(column=\"class_name\", groups=\"value_counts\")\n",
    "    .fetchdf()\n",
    "    .set_index(\"class_name\")\n",
    "    .plot.bar(rot=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28125671-5939-4e8c-b0a2-579e5ed5ad83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
